{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb8f890d",
   "metadata": {},
   "source": [
    "# Federated PyTorch CIFAR10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c19e50f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1effbc0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from openfl.interface.interactive_api.federation import Federation\n",
    "from openfl.interface.interactive_api.experiment import TaskInterface, DataInterface, ModelInterface, FLExperiment\n",
    "from copy import deepcopy\n",
    "import torchvision\n",
    "from torchvision import transforms as T\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import tqdm\n",
    "\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c8ea839",
   "metadata": {},
   "source": [
    "## Connect to the Federation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a81f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a federation\n",
    "\n",
    "# please use the same identificator that was used in signed certificate\n",
    "client_id = 'api'\n",
    "cert_dir = 'cert'\n",
    "director_node_fqdn = 'director'\n",
    "# 1) Run with API layer - Director mTLS \n",
    "# If the user wants to enable mTLS their must provide CA root chain, and signed key pair to the federation interface\n",
    "# cert_chain = f'{cert_dir}/root_ca.crt'\n",
    "# api_certificate = f'{cert_dir}/{client_id}.crt'\n",
    "# api_private_key = f'{cert_dir}/{client_id}.key'\n",
    "\n",
    "# federation = Federation(client_id=client_id, director_node_fqdn=director_node_fqdn, director_port='50051',\n",
    "#                        cert_chain=cert_chain, api_cert=api_certificate, api_private_key=api_private_key)\n",
    "\n",
    "# --------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# 2) Run with TLS disabled (trusted environment)\n",
    "# Federation can also determine local fqdn automatically\n",
    "federation = Federation(client_id=client_id, director_node_fqdn=director_node_fqdn, director_port='50051', tls=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab39e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "federation.target_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f1b5186",
   "metadata": {},
   "outputs": [],
   "source": [
    "shard_registry = federation.get_shard_registry()\n",
    "shard_registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "669db40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, request a dummy_shard_desc that holds information about the federated dataset \n",
    "dummy_shard_desc = federation.get_dummy_shard_descriptor(size=10)\n",
    "dummy_shard_dataset = dummy_shard_desc.get_dataset('train')\n",
    "sample, target = dummy_shard_dataset[0]\n",
    "print(sample.shape)\n",
    "print(target.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f650d13b",
   "metadata": {},
   "source": [
    "## Creating a FL experiment using Interactive API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f591b8d",
   "metadata": {},
   "source": [
    "### Register dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a95d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize = T.Normalize(\n",
    "    mean=[0.3037],\n",
    "    std=[0.2889]\n",
    ")\n",
    "\n",
    "augmentation = T.RandomApply(\n",
    "    [T.RandomHorizontalFlip()],\n",
    "     #T.RandomCrop(((32,32)), padding=4)],\n",
    "    p=.5\n",
    ")\n",
    "\n",
    "training_transform = T.Compose(\n",
    "    [T.ToTensor(),\n",
    "     T.Resize((32,32)),\n",
    "     #T.Grayscale(num_output_channels=1),\n",
    "     augmentation,\n",
    "     normalize]\n",
    ")\n",
    "\n",
    "valid_transform = T.Compose(\n",
    "    [T.ToTensor(),\n",
    "     T.Resize((32,32)),\n",
    "     #T.Grayscale(num_output_channels=1),\n",
    "     normalize]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c27ae0a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformedDataset(Dataset):\n",
    "    \"\"\"Image Person ReID Dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, dataset, transform=None, target_transform=None):\n",
    "        \"\"\"Initialize Dataset.\"\"\"\n",
    "        self.dataset = dataset\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Length of dataset.\"\"\"\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img, label = self.dataset[index]\n",
    "        label = self.target_transform(label) if self.target_transform else label\n",
    "        img = self.transform(img) if self.transform else img\n",
    "        return img, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b98919",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTSVHNDataset(DataInterface):\n",
    "    def __init__(self, **kwargs):\n",
    "        self.kwargs = kwargs\n",
    "    \n",
    "    @property\n",
    "    def shard_descriptor(self):\n",
    "        return self._shard_descriptor\n",
    "        \n",
    "    @shard_descriptor.setter\n",
    "    def shard_descriptor(self, shard_descriptor):\n",
    "        \"\"\"\n",
    "        Describe per-collaborator procedures or sharding.\n",
    "\n",
    "        This method will be called during a collaborator initialization.\n",
    "        Local shard_descriptor  will be set by Envoy.\n",
    "        \"\"\"\n",
    "        self._shard_descriptor = shard_descriptor\n",
    "        \n",
    "        self.train_set = TransformedDataset(\n",
    "            self._shard_descriptor.get_dataset('train'),\n",
    "            transform=training_transform\n",
    "        )\n",
    "        self.valid_set = TransformedDataset(\n",
    "            self._shard_descriptor.get_dataset('val'),\n",
    "            transform=valid_transform\n",
    "        )\n",
    "        \n",
    "    def get_train_loader(self, **kwargs):\n",
    "        \"\"\"\n",
    "        Output of this method will be provided to tasks with optimizer in contract\n",
    "        \"\"\"\n",
    "        generator=torch.Generator()\n",
    "        generator.manual_seed(0)\n",
    "        return DataLoader(\n",
    "            self.train_set, batch_size=self.kwargs['train_bs'], shuffle=True, generator=generator\n",
    "            )\n",
    "\n",
    "    def get_valid_loader(self, **kwargs):\n",
    "        \"\"\"\n",
    "        Output of this method will be provided to tasks without optimizer in contract\n",
    "        \"\"\"\n",
    "        return DataLoader(self.valid_set, batch_size=self.kwargs['valid_bs'])\n",
    "\n",
    "    def get_train_data_size(self):\n",
    "        \"\"\"\n",
    "        Information for aggregation\n",
    "        \"\"\"\n",
    "        return len(self.train_set)\n",
    "\n",
    "    def get_valid_data_size(self):\n",
    "        \"\"\"\n",
    "        Information for aggregation\n",
    "        \"\"\"\n",
    "        return len(self.valid_set)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ea1ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "fed_dataset = MNISTSVHNDataset(train_bs=64, valid_bs=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88083acc",
   "metadata": {},
   "source": [
    "### Describe the model and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1822e432",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG16(nn.Module):\n",
    "\n",
    "    def __init__(self, num_classes):\n",
    "        super(VGG16, self).__init__()\n",
    "\n",
    "        # calculate same padding:\n",
    "        # (w - k + 2*p)/s + 1 = o\n",
    "        # => p = (s(o-1) - w + k)/2\n",
    "\n",
    "        self.block_1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1,\n",
    "                      out_channels=64,\n",
    "                      kernel_size=(3, 3),\n",
    "                      stride=(1, 1),\n",
    "                      # (1(32-1)- 32 + 3)/2 = 1\n",
    "                      padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=64,\n",
    "                      out_channels=64,\n",
    "                      kernel_size=(3, 3),\n",
    "                      stride=(1, 1),\n",
    "                      padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=(2, 2),\n",
    "                         stride=(2, 2))\n",
    "        )\n",
    "\n",
    "        self.block_2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=64,\n",
    "                      out_channels=128,\n",
    "                      kernel_size=(3, 3),\n",
    "                      stride=(1, 1),\n",
    "                      padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=128,\n",
    "                      out_channels=128,\n",
    "                      kernel_size=(3, 3),\n",
    "                      stride=(1, 1),\n",
    "                      padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=(2, 2),\n",
    "                         stride=(2, 2))\n",
    "        )\n",
    "        \n",
    "        self.block_3 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=128,\n",
    "                      out_channels=256,\n",
    "                      kernel_size=(3, 3),\n",
    "                      stride=(1, 1),\n",
    "                      padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=256,\n",
    "                      out_channels=256,\n",
    "                      kernel_size=(3, 3),\n",
    "                      stride=(1, 1),\n",
    "                      padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=256,\n",
    "                      out_channels=256,\n",
    "                      kernel_size=(3, 3),\n",
    "                      stride=(1, 1),\n",
    "                      padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=(2, 2),\n",
    "                         stride=(2, 2))\n",
    "        )\n",
    "\n",
    "        self.block_4 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=256,\n",
    "                      out_channels=512,\n",
    "                      kernel_size=(3, 3),\n",
    "                      stride=(1, 1),\n",
    "                      padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=512,\n",
    "                      out_channels=512,\n",
    "                      kernel_size=(3, 3),\n",
    "                      stride=(1, 1),\n",
    "                      padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=512,\n",
    "                      out_channels=512,\n",
    "                      kernel_size=(3, 3),\n",
    "                      stride=(1, 1),\n",
    "                      padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=(2, 2),\n",
    "                         stride=(2, 2))\n",
    "        )            \n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(2048, 4096),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(p=0.65),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(p=0.65),\n",
    "            nn.Linear(4096, num_classes) \n",
    "        )\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, torch.nn.Conv2d) or isinstance(m, torch.nn.Linear):\n",
    "                nn.init.kaiming_uniform_(m.weight, mode='fan_in', nonlinearity='leaky_relu')\n",
    "                if m.bias is not None:\n",
    "                    m.bias.detach().zero_()\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.block_1(x)\n",
    "        x = self.block_2(x)\n",
    "        x = self.block_3(x)\n",
    "        x = self.block_4(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "model = VGG16(10)\n",
    "\n",
    "model.block_1[1] = nn.GroupNorm(32, 64)\n",
    "model.block_1[4] = nn.GroupNorm(32, 64)\n",
    "model.block_2[1] = nn.GroupNorm(32, 128)\n",
    "model.block_2[4] = nn.GroupNorm(32, 128)\n",
    "model.block_3[1] = nn.GroupNorm(32, 256)\n",
    "model.block_3[4] = nn.GroupNorm(32, 256)\n",
    "model.block_3[7] = nn.GroupNorm(32, 256)\n",
    "model.block_4[1] = nn.GroupNorm(32, 512)\n",
    "model.block_4[4] = nn.GroupNorm(32, 512)\n",
    "model.block_4[7] = nn.GroupNorm(32, 512)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b1bd779",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_net = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24757acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210a546e",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(model_net.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9064c37e",
   "metadata": {},
   "outputs": [],
   "source": [
    "params_to_update = []\n",
    "for param in model_net.parameters():\n",
    "    if param.requires_grad == True:\n",
    "        params_to_update.append(param)\n",
    "'''\n",
    "FEDPROX\n",
    "'''        \n",
    "#from openfl.utilities.optimizers.torch import FedProxAdam        \n",
    "#optimizer = FedProxAdam(params_to_update, lr=1e-4, mu=0.01)\n",
    "\n",
    "'''\n",
    "ORIGINALE\n",
    "'''\n",
    "optimizer = optim.Adam(params_to_update, lr=1e-4)\n",
    "#optimizer = optim.AdamW(params_to_update, lr=0.001, weight_decay=0.02)\n",
    "#optimizer = optim.SGD(params_to_update, lr=0.01)\n",
    "\n",
    "#scheduler\n",
    "#scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
    "\n",
    "def cross_entropy(output, target):\n",
    "    \"\"\"Cross-entropy metric\n",
    "    \"\"\"\n",
    "    #return F.cross_entropy(input=output,target=target)\n",
    "    #return F.binary_cross_entropy_with_logits(input=output,target=target)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    loss = criterion(output, target)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b4d3b54",
   "metadata": {},
   "source": [
    "### Register model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b1d758",
   "metadata": {},
   "outputs": [],
   "source": [
    "framework_adapter = 'openfl.plugins.frameworks_adapters.pytorch_adapter.FrameworkAdapterPlugin'\n",
    "model_interface = ModelInterface(model=model_net, optimizer=optimizer, framework_plugin=framework_adapter)\n",
    "\n",
    "# Save the initial model state\n",
    "initial_model = deepcopy(model_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d322eb",
   "metadata": {},
   "source": [
    "## Define and register FL tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b02acc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "task_interface = TaskInterface()\n",
    "\n",
    "'''\n",
    "FEDCURV\n",
    "'''\n",
    "#from openfl.utilities.fedcurv.torch import FedCurv\n",
    "#from openfl.component.aggregation_functions import FedCurvWeightedAverage\n",
    "#import tqdm\n",
    "\n",
    "#fedcurv = FedCurv(model_interface.provide_model(), importance=1e3)\n",
    "\n",
    "'''\n",
    "FEDOPT\n",
    "'''\n",
    "\n",
    "#from openfl.component.aggregation_functions import AdagradAdaptiveAggregation    \n",
    "#agg_fn = AdagradAdaptiveAggregation(model_interface=model_interface, learning_rate=0.4)     \n",
    "#@task_interface.set_aggregation_function(agg_fn)\n",
    "\n",
    "\n",
    "# The Interactive API supports registering functions definied in main module or imported.\n",
    "def function_defined_in_notebook(some_parameter):\n",
    "    print(f'Also I accept a parameter and it is {some_parameter}')\n",
    "\n",
    "# Task interface currently supports only standalone functions.\n",
    "@task_interface.add_kwargs(**{'some_parameter': 42})\n",
    "@task_interface.register_fl_task(model='net_model', data_loader='train_loader', \\\n",
    "                     device='device', optimizer='optimizer') \n",
    "#@task_interface.set_aggregation_function(FedCurvWeightedAverage())\n",
    "\n",
    "\n",
    "def train(net_model, train_loader, optimizer, device, loss_fn=cross_entropy, some_parameter=None):\n",
    "    torch.manual_seed(0)\n",
    "    #fedcurv.on_train_begin(net_model)\n",
    "    device='cuda'\n",
    "    function_defined_in_notebook(some_parameter)\n",
    "    \n",
    "    train_loader = tqdm.tqdm(train_loader, desc=\"train\")\n",
    "    net_model.train()\n",
    "    net_model.to(device)\n",
    "\n",
    "    losses = []\n",
    "    epochs = 2\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        for data, target in train_loader:\n",
    "            data, target = torch.tensor(data).to(device), torch.tensor(\n",
    "                target).to(device, dtype=torch.int64)\n",
    "            optimizer.zero_grad()\n",
    "            #data = data.type(torch.LongTensor)\n",
    "            #target = target.type(torch.LongTensor)\n",
    "            output = net_model(data)\n",
    "            #output = output.logits #per GOOGLENET\n",
    "            loss = loss_fn(output=output, target=target) #+ fedcurv.get_penalty(net_model)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            losses.append(loss.detach().cpu().numpy())\n",
    "    #fedcurv.on_train_end(net_model, train_loader, device)    \n",
    "    return {'train_loss': np.mean(losses),}\n",
    "\n",
    "\n",
    "@task_interface.register_fl_task(model='net_model', data_loader='val_loader', device='device')     \n",
    "def validate(net_model, val_loader, device):\n",
    "    torch.manual_seed(0)\n",
    "    device = torch.device('cuda')\n",
    "    net_model.eval()\n",
    "    net_model.to(device)\n",
    "    \n",
    "    val_loader = tqdm.tqdm(val_loader, desc=\"validate\")\n",
    "    val_score = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, target in val_loader:\n",
    "            samples = target.shape[0]\n",
    "            total_samples += samples\n",
    "            data, target = torch.tensor(data).to(device), \\\n",
    "                torch.tensor(target).to(device, dtype=torch.int64)\n",
    "            output = net_model(data)\n",
    "            #da wine\n",
    "            #_, preds = torch.max(outputs, dim=1)\n",
    "            #return torch.tensor(torch.sum(preds == labels).item() / len(preds))\n",
    "            \n",
    "            #originale\n",
    "            #pred = output.argmax(dim=1,keepdim=True)\n",
    "            \n",
    "            #tentativo\n",
    "            _, pred = torch.max(output, dim=1)\n",
    "            val_score += pred.eq(target).sum().cpu().numpy()\n",
    "            \n",
    "    return {'acc': val_score / total_samples,}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f802b74e",
   "metadata": {},
   "source": [
    "## Time to start a federated learning experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f18dd63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an experimnet in federation\n",
    "experiment_name = 'mnist_svhn_VGG16GN_FederatedStreamflow_50rounds_2epoch'\n",
    "fl_experiment = FLExperiment(federation=federation, experiment_name=experiment_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7501d9e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following command zips the workspace and python requirements to be transfered to collaborator nodes\n",
    "fl_experiment.start(\n",
    "    model_provider=model_interface, \n",
    "    task_keeper=task_interface,\n",
    "    data_loader=fed_dataset,\n",
    "    rounds_to_train=50,\n",
    "    opt_treatment='CONTINUE_GLOBAL'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c17a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If user want to stop IPython session, then reconnect and check how experiment is going\n",
    "# fl_experiment.restore_experiment_state(model_interface)\n",
    "\n",
    "fl_experiment.stream_metrics(tensorboard_logs=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
